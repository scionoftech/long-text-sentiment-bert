{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Text Sentiment - Sentence Sentiment Mean\n",
    "\n",
    "So far, we have restricted the length of the text being fed into our models. Bert in particular is restricted to consuming 512 tokens per sample. For many use-cases, this is most likely not a problem - but in some cases it can be.\n",
    "\n",
    "If we take the example of Customer feedbacks on e-commerce sites, which often consists of what customers is thinking about the products. On these longer pieces of text, the actual sentiment from the customer may not be clear from the first 512 tokens. We need to consider the full post.\n",
    "\n",
    "Before working through the logic that allows us to consider the full post, let's import and define everything we need to make a prediction on a single chunk of text (using much of what we covered in the last section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this text is taken from kaggle Amazon Reviews Dataset\n",
    "# https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "text = \"\"\"\n",
    "       One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I have only played a small portion of the game,\n",
    "       the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my \n",
    "       favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, \n",
    "       as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) \n",
    "       has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the \n",
    "       songs, which I find distracting. But even if those weren't included I would still consider the collection worth it. Not an \"ultimate guide\": Firstly,I enjoyed the format and tone of the book (how the author addressed the reader). \n",
    "       However, I did not feel that she imparted any insider secrets that the book promised to reveal. \n",
    "       If you are just starting to research law school, and do not know all the requirements of admission, then this book may be a tremendous help. \n",
    "       If you have done your homework and are looking for an edge when it comes to admissions, I recommend some more topic-specific books. \n",
    "       For example, books on how to write your personal statment, books geared specifically towards LSAT preparation (Powerscore books were the most helpful for me), and there are some websites with great advice geared towards aiding the individuals whom you are asking to write letters of recommendation. \n",
    "       Yet, for those new to the entire affair, this book can definitely clarify the requirements for you. Don't Take the Chance - Get the SE Branded Cable: If you purchase this data cable, you need to know that you will receive no real directions or information regarding what to check if nothing works. As directed, I downloaded all of the files from the SE site (70MB on dial up!), and then downloaded all of the user guides.\n",
    "       Everything seemed to install ok, but nothing would make my phone be recognized. After that I scoured the SE site for troubleshooting info on their branded cable-in the hope that something would help me figure out the problem. \n",
    "       After 2 full days of beating my head against the wall, I finally threw the cable and the useless CD that came with it in the trash.If I had used my brain I would have paid the extra $$ for a SE branded cable and software (and the support that comes along with that). \n",
    "       I now have the real deal (SE data cable and software), and guess what? Yep, installation was a breeze and it works beautifully. You really do get what you pay for. great IMO: First of all, I saw the review by \"Tyley Mike \"Relite\"\" and thought he was grossly overcritical of EVERYTHING and every instrument played... \n",
    "       so I'd like to hear Tyley Mike's album, since he thinks he can do better :) --seriously! I think some people don't understand that things sound the way they were MEANT to sound, if they sound poppy, they made it that way, why the hell should they stick to the norm? They want to do something different and in my opinion it sounds great.\n",
    "       I can't write a good enough review for this album, all their albums actually, as they are all a masterpiece of their own while still being different enough to keep it interesting. It bugs me when a group doesn't evolve or try new things and stays exactly the same as they ever were, all the time, so I was glad to see them progress and \"grow\".\n",
    "       There's too much to say to describe this album, but frankly I don't think I could write a good enough review to do it justice, so I'll just give it my 5 stars :) .  It Rises above the \"Fluff\" Books: The first thing that struck me was that it was easy to read. \n",
    "       The print was readable and the illustrations were helpful. I did also find some grammatical errors as an earlier review said. But mostly it was very specific and practical. \n",
    "       The chapters most helpful were on \"emotional states\" and music. It's hard to find a book on this subject that's across the board, dealing with many different issues and this one addresses nearly every brain-related research issue from nutrition to memory. \n",
    "       As a scientist who also works with high school students, I found his translation of brain research into the classroom to be thoughtful, if not enthusiastic. It's a tough subject to translate, but I did get more than I thought I would out of the book.\n",
    "       Mostly it helped me get past the hype and get into the real practical meat of the material. The book's far from perfect, but it's the best I've seen so far on this topic. \n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to how we apply sentiment to longer pieces of text. this approch split the text into sentences and calculates the mean of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bertweet-base-sentiment-analysis')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bertweet-base-sentiment-analysis')\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    # get tokens\n",
    "    inputs = tokenizer.encode_plus(text,return_tensors='pt')\n",
    "    # get output logits from the model\n",
    "    output = model(**inputs)\n",
    "    # convert to probabilities\n",
    "    probs = torch.nn.functional.softmax(output[0], dim=-1)\n",
    "    # we will return the probability tensor (we will not need argmax until later)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the text token length\n",
    "tokens = tokenizer.encode_plus(text, add_special_tokens=False)\n",
    "\n",
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tokenize this longer piece of text we get a total of **1046** tokens, far too many to fit into our BERT model containing a maximum limit of 512 tokens. We will need to split this text into sentences, and calculate our sentiment probabilities for each sentences seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into sentences\n",
    "sentences = text.split(\".\")\n",
    "# initialize probabilities list\n",
    "probs_list = []\n",
    "# loop through each sentence\n",
    "for sentence in sentences:\n",
    "    probs_list.append(get_sentiment(text=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[7.9904e-04, 1.6766e-02, 9.8244e-01]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0017, 0.0061, 0.9922]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0242, 0.3864, 0.5894]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1582, 0.7451, 0.0967]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9796, 0.0171, 0.0032]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0118, 0.9398, 0.0484]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0019, 0.1991, 0.7989]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4761, 0.5177, 0.0062]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0017, 0.1019, 0.8964]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0037, 0.8373, 0.1590]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[7.8438e-04, 3.4438e-02, 9.6478e-01]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0017, 0.7239, 0.2744]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4443, 0.5429, 0.0128]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0024, 0.9386, 0.0590]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0390, 0.9314, 0.0296]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1518, 0.8017, 0.0465]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9829, 0.0134, 0.0038]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.3120, 0.6675, 0.0205]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0012, 0.0070, 0.9918]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1997, 0.7796, 0.0207]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2847, 0.6726, 0.0428]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0316, 0.8137, 0.1547]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0316, 0.8137, 0.1547]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4410, 0.5310, 0.0280]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0013, 0.0573, 0.9414]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.7573, 0.2256, 0.0171]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0033, 0.3743, 0.6224]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0024, 0.4961, 0.5015]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[9.3977e-04, 1.5687e-02, 9.8337e-01]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.8571, 0.1394, 0.0035]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0020, 0.8473, 0.1507]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0012, 0.1555, 0.8433]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.2753, 0.7068, 0.0179]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.1359, 0.8217, 0.0425]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0630, 0.9150, 0.0220]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0011, 0.0990, 0.8998]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0135, 0.2914, 0.6952]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0316, 0.8137, 0.1547]], grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section has been assign varying levels of sentiment. To calculate the average sentiment across the full text, we will merge these tensors using the `stack` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the mean score of each column (positive, negative, and neutral sentiment respectively) using `mean(dim=0)`. But before we do that we must reshape our tensor into a *3x3* shape - it is currently a 3x1x3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take mean of probabilities\n",
    "with torch.no_grad():\n",
    "    # we must include our stacks operation in here too\n",
    "    stacks = torch.stack(probs_list)\n",
    "    print(stacks.shape)\n",
    "    # We can reshape our tensor dimensions using the `resize_` method, and use dimensions `0` and `2` of our current tensor shape\n",
    "    print(shape[0], shape[2])\n",
    "    # now resize\n",
    "    stacks = stacks.resize_(stacks.shape[0], stacks.shape[2])\n",
    "    # finally, we can calculate the mean value for each sentiment class\n",
    "    mean = stacks.mean(dim=0)\n",
    "    scores = mean.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17711022, 0.4736138 , 0.3492761 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) neutral 0.4736\n",
      "2) positive 0.3493\n",
      "3) negative 0.1771\n"
     ]
    }
   ],
   "source": [
    "# sentiment\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "senti_obj = list()\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i + 1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
