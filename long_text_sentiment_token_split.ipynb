{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Text Sentiment - Text Token Split Mean\n",
    "\n",
    "So far, we have restricted the length of the text being fed into our models. Bert in particular is restricted to consuming 512 tokens per sample. For many use-cases, this is most likely not a problem - but in some cases it can be.\n",
    "\n",
    "If we take the example of Customer feedbacks on e-commerce sites, which often consists of what customers is thinking about the products. On these longer pieces of text, the actual sentiment from the customer may not be clear from the first 512 tokens. We need to consider the full post.\n",
    "\n",
    "Before working through the logic that allows us to consider the full post, let's import and define everything we need to make a prediction on a single chunk of text (using much of what we covered in the last section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this text is taken from kaggle Amazon Reviews Dataset\n",
    "# https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "text = \"\"\"\n",
    "       One of the best game music soundtracks - for a game I didn't really play: Despite the fact that I have only played a small portion of the game,\n",
    "       the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my \n",
    "       favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, \n",
    "       as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) \n",
    "       has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the \n",
    "       songs, which I find distracting. But even if those weren't included I would still consider the collection worth it. Not an \"ultimate guide\": Firstly,I enjoyed the format and tone of the book (how the author addressed the reader). \n",
    "       However, I did not feel that she imparted any insider secrets that the book promised to reveal. \n",
    "       If you are just starting to research law school, and do not know all the requirements of admission, then this book may be a tremendous help. \n",
    "       If you have done your homework and are looking for an edge when it comes to admissions, I recommend some more topic-specific books. \n",
    "       For example, books on how to write your personal statment, books geared specifically towards LSAT preparation (Powerscore books were the most helpful for me), and there are some websites with great advice geared towards aiding the individuals whom you are asking to write letters of recommendation. \n",
    "       Yet, for those new to the entire affair, this book can definitely clarify the requirements for you. Don't Take the Chance - Get the SE Branded Cable: If you purchase this data cable, you need to know that you will receive no real directions or information regarding what to check if nothing works. As directed, I downloaded all of the files from the SE site (70MB on dial up!), and then downloaded all of the user guides.\n",
    "       Everything seemed to install ok, but nothing would make my phone be recognized. After that I scoured the SE site for troubleshooting info on their branded cable-in the hope that something would help me figure out the problem. \n",
    "       After 2 full days of beating my head against the wall, I finally threw the cable and the useless CD that came with it in the trash.If I had used my brain I would have paid the extra $$ for a SE branded cable and software (and the support that comes along with that). \n",
    "       I now have the real deal (SE data cable and software), and guess what? Yep, installation was a breeze and it works beautifully. You really do get what you pay for. great IMO: First of all, I saw the review by \"Tyley Mike \"Relite\"\" and thought he was grossly overcritical of EVERYTHING and every instrument played... \n",
    "       so I'd like to hear Tyley Mike's album, since he thinks he can do better :) --seriously! I think some people don't understand that things sound the way they were MEANT to sound, if they sound poppy, they made it that way, why the hell should they stick to the norm? They want to do something different and in my opinion it sounds great.\n",
    "       I can't write a good enough review for this album, all their albums actually, as they are all a masterpiece of their own while still being different enough to keep it interesting. It bugs me when a group doesn't evolve or try new things and stays exactly the same as they ever were, all the time, so I was glad to see them progress and \"grow\".\n",
    "       There's too much to say to describe this album, but frankly I don't think I could write a good enough review to do it justice, so I'll just give it my 5 stars :) .  It Rises above the \"Fluff\" Books: The first thing that struck me was that it was easy to read. \n",
    "       The print was readable and the illustrations were helpful. I did also find some grammatical errors as an earlier review said. But mostly it was very specific and practical. \n",
    "       The chapters most helpful were on \"emotional states\" and music. It's hard to find a book on this subject that's across the board, dealing with many different issues and this one addresses nearly every brain-related research issue from nutrition to memory. \n",
    "       As a scientist who also works with high school students, I found his translation of brain research into the classroom to be thoughtful, if not enthusiastic. It's a tough subject to translate, but I did get more than I thought I would out of the book.\n",
    "       Mostly it helped me get past the hype and get into the real practical meat of the material. The book's far from perfect, but it's the best I've seen so far on this topic. \n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to how we apply sentiment to longer pieces of text. this approch split the text into sentences and calculates the mean of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bertweet-base-sentiment-analysis')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bertweet-base-sentiment-analysis')\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    # get tokens\n",
    "    inputs = tokenizer.encode_plus(text,return_tensors='pt')\n",
    "    # get output logits from the model\n",
    "    output = model(**inputs)\n",
    "    # convert to probabilities\n",
    "    probs = torch.nn.functional.softmax(output[0], dim=-1)\n",
    "    # we will return the probability tensor (we will not need argmax until later)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the text token length\n",
    "tokens = tokenizer.encode_plus(text, add_special_tokens=False)\n",
    "\n",
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tokenize this longer piece of text we get a total of **1046** tokens, far too many to fit into bertweet-base-sentiment-analysis model containing a maximum limit of 128 tokens. We will need to split this text into chunks of 128 tokens at a time, and calculate our sentiment probabilities for each chunk seperately.\n",
    "\n",
    "Because we are taking this slightly different approach, we have encoded our tokens using a different set of parameters to what we have used before. This time, we:\n",
    "\n",
    "* Avoided adding special tokens `add_special_tokens=False` because this will add *[CLS]* and *[SEP]* tokens to the start and end of the full tokenized tensor of length **1046**, we will instead add them manually later.\n",
    "\n",
    "* We will not specify `max_length`, `truncation`, or `padding` parameters (as we do not use any of them here).\n",
    "\n",
    "* We will return standard Python *lists* rather than tensors by not specifying `return_tensors` (it will return lists by default). This will make the following logic steps easier to follow - but we will rewrite them using PyTorch code in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we break our tokenized dictionary into `input_ids` and `attention_mask` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access slices of these lists like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6227, 6, 733, 25, 8, 36, 121, 834, 11, 915, 10705, 15, 6, 20905, 6458, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[16:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=0\n",
      "end=128\n",
      "start=128\n",
      "end=256\n",
      "start=256\n",
      "end=384\n",
      "start=384\n",
      "end=512\n",
      "start=512\n",
      "end=640\n",
      "start=640\n",
      "end=768\n",
      "start=768\n",
      "end=896\n",
      "start=896\n",
      "end=1024\n",
      "start=1024\n",
      "end=1046\n"
     ]
    }
   ],
   "source": [
    "# define our starting position (0) and window size (number of tokens in each chunk)\n",
    "start = 0\n",
    "window_size = 128\n",
    "\n",
    "# get the total length of our tokens\n",
    "total_len = len(input_ids)\n",
    "\n",
    "# initialize condition for our while loop to run\n",
    "loop = True\n",
    "\n",
    "# loop through and print out start/end positions\n",
    "while loop:\n",
    "    # the end position is simply the start + window_size\n",
    "    end = start + window_size\n",
    "    # if the end position is greater than the total length, make this our final iteration\n",
    "    if end >= total_len:\n",
    "        loop = False\n",
    "        # and change our endpoint to the final token position\n",
    "        end = total_len\n",
    "    print(f\"start={start}\\nend={end}\")\n",
    "    # we need to move the window to the next 512 tokens\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logic works for shifting our window across the full length of input IDs, so now we can modify it to iterately predict sentiment for each window. There will be a few added steps for us to get this to work:\n",
    "\n",
    "1. Extract the window from `input_ids` and `attention_mask`.\n",
    "\n",
    "2. Add the start of sequence token `[CLS]`/`101` and seperator token `[SEP]`/`102`.\n",
    "\n",
    "3. Add padding (only applicable to final batch).\n",
    "\n",
    "4. Format into dictionary containing PyTorch tensors.\n",
    "\n",
    "5. Make logits predictions with the model.\n",
    "\n",
    "6. Calculate softmax and append softmax vector to a list `probs_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0010, 0.0098, 0.9892]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4240, 0.5489, 0.0272]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0010, 0.0867, 0.9123]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9362, 0.0607, 0.0031]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.7842, 0.1430, 0.0729]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.5739, 0.4029, 0.0232]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0079, 0.3190, 0.6731]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0027, 0.4297, 0.5676]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0030, 0.1675, 0.8295]], grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize probabilities list\n",
    "probs_list = []\n",
    "\n",
    "start = 0\n",
    "window_size = 126  # we take 2 off here so that we can fit in our [CLS] and [SEP] tokens\n",
    "\n",
    "loop = True\n",
    "\n",
    "while loop:\n",
    "    end = start + window_size\n",
    "    if end >= total_len:\n",
    "        loop = False\n",
    "        end = total_len\n",
    "    # (1) extract window from input_ids and attention_mask\n",
    "    input_ids_chunk = input_ids[start:end]\n",
    "    attention_mask_chunk = attention_mask[start:end]\n",
    "    # (2) add [CLS] and [SEP]\n",
    "    input_ids_chunk = [101] + input_ids_chunk + [102]\n",
    "    attention_mask_chunk = [1] + attention_mask_chunk + [1]\n",
    "    # (3) add padding upto window_size + 2 (512) tokens\n",
    "    input_ids_chunk += [0] * (window_size - len(input_ids_chunk) + 2)\n",
    "    attention_mask_chunk += [0] * (window_size - len(attention_mask_chunk) + 2)\n",
    "    # (4) format into PyTorch tensors dictionary\n",
    "    input_dict = {\n",
    "        'input_ids': torch.Tensor([input_ids_chunk]).long(),\n",
    "        'attention_mask': torch.Tensor([attention_mask_chunk]).int()\n",
    "    }\n",
    "    print(len(input_ids_chunk))\n",
    "    print(len(attention_mask_chunk))\n",
    "    # (5) make logits prediction\n",
    "    outputs = model(**input_dict)\n",
    "    # (6) calculate softmax and append to list\n",
    "    probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    probs_list.append(probs)\n",
    "\n",
    "    start = end\n",
    "    \n",
    "# let's view the probabilities given\n",
    "probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0010, 0.0098, 0.9892]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.4240, 0.5489, 0.0272]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0010, 0.0867, 0.9123]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.9362, 0.0607, 0.0031]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.7842, 0.1430, 0.0729]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.5739, 0.4029, 0.0232]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0079, 0.3190, 0.6731]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0027, 0.4297, 0.5676]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[0.0030, 0.1675, 0.8295]], grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section has been assign varying levels of sentiment. To calculate the average sentiment across the full text, we will merge these tensors using the `stack` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate the mean score of each column (positive, negative, and neutral sentiment respectively) using `mean(dim=0)`. But before we do that we must reshape our tensor into a *3x3* shape - it is currently a 3x1x3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1, 3])\n",
      "9 3\n"
     ]
    }
   ],
   "source": [
    "# take mean of probabilities\n",
    "with torch.no_grad():\n",
    "    # we must include our stacks operation in here too\n",
    "    stacks = torch.stack(probs_list)\n",
    "    print(stacks.shape)\n",
    "    # We can reshape our tensor dimensions using the `resize_` method, and use dimensions `0` and `2` of our current tensor shape\n",
    "    print(stacks.shape[0], stacks.shape[2])\n",
    "    # now resize\n",
    "    stacks = stacks.resize_(stacks.shape[0], stacks.shape[2])\n",
    "    # finally, we can calculate the mean value for each sentiment class\n",
    "    mean = stacks.mean(dim=0)\n",
    "    scores = mean.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30374646, 0.24089938, 0.4553542 ], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.4554\n",
      "2) negative 0.3037\n",
      "3) neutral 0.2409\n"
     ]
    }
   ],
   "source": [
    "# sentiment\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "senti_obj = list()\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i + 1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
